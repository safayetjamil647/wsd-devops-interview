{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WSD Devops Interview Challenge Case and Scenarios Ansible and Puppet 1: Which ansible command can display all ansible_ configuration for a host. Ans: ansible <host> -m setup -i inventory.yml For my case for challenge number 3 with ansible the answer will be like ansible app-vm -m setup -i inventory.yml 2: Please configure a cron job that runs logrotate on all machines every 10 minutes between 2h - 4h. 3: Please deploy ntpd package to the following 3 servers with custom config of /etc/ntpd.conf and We also need to deploy monitoring template onto our nagios server \u201cmonitoring.fra1.internal\u201d, each of the above machines should use the following nagios templates: Ans: This github reference url contains the Ansible playbook to deploy the ntpd service with custom configurations on three remote servers. Additionally, each server is registered with the Nagios monitoring server using specific templates and nagios host configurations What This Playbook Does Installs the ntp , nagios-nrpe-server , nagios-plugins package on all target hosts. Replaces /etc/ntp.conf with a custom configuration. Restarts the NTP daemon. Configures NRPE checks for NTP and ping on all nodes. Ensures the Nagios server applies the following templates per host: ntp-monitoring ping-monitoring Docker/Kubernetes Suggested environment: Ubuntu 20 LTS, docker 19 or above 1:Prepare a docker-compose for a nginx server. Requirements: nginx logs need to survive between nginx container restarts docker should use network bridge subnet 172.20.8.0/24 Ans : Nginx with Docker Compose View on GitHub Requirements Overview 1. Log Persistence and network configurations To ensure logs survive between container restarts, a volume is mounted and to achive the network configurations pls check the network block in the yml . version: '3.8' services: nginx: image: nginx:alpine container_name: nginx restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./logs:/var/log/nginx networks: nginx_net: ipv4_address: 172.20.8.10 networks: nginx_net: driver: bridge ipam: config: - subnet: 172.20.8.0/24 gateway: 172.20.8.1 View on GitHub 2: Which Kubernetes command you will use to identify the reason for a pod restart in the project \"internal\" under namespace \"production\". Ans: kubectl get pods -n production -l app=internal 3:Case for pod restart issue: Memory request & limit: 1000 & 1500 CPU request & limit: 1000 & 2000 Xmx of 1000M Java-app keep restarting at random. From Kubernetes configuration perspective, what are the possible reasons for the pod restarts? Ans: OOMKill (Out of Memory) Even though Xmx is set to 1000M, Java memory usage is not limited to heap alone. Native memory (e.g., metaspace, threads, GC, off-heap caches) can exceed this, possibly hitting the pod's memory limit (1500Mi) and triggering OOMKilled. Also there are other possible reason like CPU Throttling and Logrotate Crush Loop or Native Memory Leaks of JVM Helm Please use the accompanied elasticsearch helm template to create a Kubernetes deployment of elasticsearch. Provide a screenshot & deployment yaml of the resultant deployment in Kubernetes. Elasticsearch Deployment on Kubernetes via Helm This guide demonstrates how to deploy Elasticsearch using a Helm chart and provides both: The final rendered Deployment YAML A screenshot of the deployed pod in Kubernetes A reference to the GitHub-hosted Helm chart template GitHub Reference Helm chart location: View elasticsearch Helm chart on GitHub Metrics Explain how Prometheus work. Ans : Prometheus is tool which uses time series database to scrapes/pulls data from exporters and store it for visualizations and alerting or other type of customizations . Scrape metrics from exporters Prometheus collects metrics from sources like: node_exporter (system metrics) blackbox_exporter (probe endpoints) Store metrics in a time-series database Prometheus stores data in a high-performance, multi-dimensional time-series DB. Labels (key-value pairs) enable powerful filtering and grouping. Pull-based architecture Prometheus pulls metrics from targets. Requires configuration on host machines to expose metrics endpoints (e.g., port 9100 for node_exporter ). How do you create custom Prometheus alerts and alerting rules for Kubernetes monitoring? Provide an example alert rule and its configuration. Below there an example alerts rules for kubernetes monitoring . In that rules it is specifically checking nodes availability of kubernetes cluster. - alert: KubernetesNodeNotReady expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0 for: 10m labels: severity: critical annotations: summary: Kubernetes Node not ready (instance {{ $labels.instance }}) description: \"Node {{ $labels.node }} has been unready for a long time\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" Also another example alert for previously shared java application issue - alert: KubernetesContainerOomKiller expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1 for: 0m labels: severity: warning annotations: summary: Kubernetes Container oom killer (instance {{ $labels.instance }}) description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" What is the Prometheus query you can use in Granfana to properly show usage trend of an application metric that is a counter? Ans: rate(http_requests_total[5m]) Cassandra Case:Query to db cluster returns different result each time. Users reported query result has data records that they deleted days ago. Explain what the likely reason for the behavior and how to avoid it. Ans: In Cassandra: A DELETE doesn\u2019t remove data immediately.Instead, it writes a tombstone marker.During compaction, tombstones and old data are reconciled and removed.If compaction hasn\u2019t happened, stale data might still exist on disk and be served if consistency is low. Adjust gc_grace_seconds Appropriately Default: 86400 seconds (1 days). Recommendation: Only reduce this value if you're confident that all replicas are consistently up and that repairs are run more frequently than the new gc_grace_seconds value. Setting it too low can risk zombie data if a node misses a tombstone due to being down . How to Set: ALTER TABLE keyspace.table_name WITH gc_grace_seconds = 86400; Ensure Regular RepairsRun nodetool repair on all nodes within the gc_grace_seconds timeframe to synchronize data and tombstones across replicas.This prevents scenarios where a node misses a deletion and later resurrects data . Use Appropriate Consistency Levels. Writes: Use QUORUM or higher to ensure that a majority of replicas acknowledge the write. Reads: Use QUORUM or higher to ensure that reads fetch the most recent data from multiple replicas, reducing the chance of reading stale data. Mongo Case We have a MongoDB replica set called replicaset_1 containing the database sanfrancisco . Collection: sanfrancisco.company_name Due to performance limitations , we've added a second replica set: replicaset_2 Goal: Shard the company_name collection across both replica sets using _id as the shard key Prerequisites Ensure: - Sharding is enabled on your MongoDB cluster - Replica sets replicaset_1 and replicaset_2 are added as shards - You are connected to the mongos router and config server is properly running with proper configurations Steps to Shard the Collection sh.enableSharding(\"sanfrancisco\") sh.addShard(\"rs-shard-01/shard-01-node-a:27017,shard-01-node-b:27017,shard-01-node-c:27017\") sh.addShard(\"rs-shard-02/shard-02-node-a:27017,shard-02-node-b:27017,shard-02-node-c:27017\") sh.enableSharding(\"sanfrancisco\") sh.shardCollection(\"sanfrancisco.company_name\", { _id: \"hashed\" })","title":"Welcome to WSD Devops Interview Challenge"},{"location":"#welcome-to-wsd-devops-interview-challenge","text":"","title":"Welcome to WSD Devops Interview Challenge"},{"location":"#case-and-scenarios","text":"","title":"Case and Scenarios"},{"location":"#ansible-and-puppet","text":"","title":"Ansible and Puppet"},{"location":"#1-which-ansible-command-can-display-all-ansible_-configuration-for-a-host","text":"Ans: ansible <host> -m setup -i inventory.yml For my case for challenge number 3 with ansible the answer will be like ansible app-vm -m setup -i inventory.yml","title":"1: Which ansible command can display all ansible_ configuration for a host."},{"location":"#2-please-configure-a-cron-job-that-runs-logrotate-on-all-machines-every-10-minutes-between-2h-4h","text":"","title":"2: Please configure a cron job that runs logrotate on all machines every 10 minutes between 2h - 4h."},{"location":"#3-please-deploy-ntpd-package-to-the-following-3-servers-with-custom-config-of-etcntpdconf-and-we-also-need-to-deploy-monitoring-template-onto-our-nagios-server-monitoringfra1internal-each-of-the-above-machines-should-use-the-following-nagios-templates","text":"Ans: This github reference url contains the Ansible playbook to deploy the ntpd service with custom configurations on three remote servers. Additionally, each server is registered with the Nagios monitoring server using specific templates and nagios host configurations What This Playbook Does Installs the ntp , nagios-nrpe-server , nagios-plugins package on all target hosts. Replaces /etc/ntp.conf with a custom configuration. Restarts the NTP daemon. Configures NRPE checks for NTP and ping on all nodes. Ensures the Nagios server applies the following templates per host: ntp-monitoring ping-monitoring","title":"3: Please deploy ntpd package to the following 3 servers with custom config of /etc/ntpd.conf and We also need to deploy monitoring template onto our nagios server \u201cmonitoring.fra1.internal\u201d, each of the above machines should use the following nagios templates:"},{"location":"#dockerkubernetes","text":"Suggested environment: Ubuntu 20 LTS, docker 19 or above 1:Prepare a docker-compose for a nginx server. Requirements: nginx logs need to survive between nginx container restarts docker should use network bridge subnet 172.20.8.0/24 Ans :","title":"Docker/Kubernetes"},{"location":"#nginx-with-docker-compose","text":"View on GitHub","title":"Nginx with Docker Compose"},{"location":"#requirements-overview","text":"","title":"Requirements Overview"},{"location":"#1-log-persistence-and-network-configurations","text":"To ensure logs survive between container restarts, a volume is mounted and to achive the network configurations pls check the network block in the yml . version: '3.8' services: nginx: image: nginx:alpine container_name: nginx restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./logs:/var/log/nginx networks: nginx_net: ipv4_address: 172.20.8.10 networks: nginx_net: driver: bridge ipam: config: - subnet: 172.20.8.0/24 gateway: 172.20.8.1 View on GitHub","title":"1. Log Persistence and network configurations"},{"location":"#2-which-kubernetes-command-you-will-use-to-identify-the-reason-for-a-pod-restart-in-the-project-internal-under-namespace-production","text":"Ans: kubectl get pods -n production -l app=internal","title":"2: Which Kubernetes command you will use to identify the reason for a pod restart in the project \"internal\" under namespace \"production\"."},{"location":"#3case-for-pod-restart-issue","text":"Memory request & limit: 1000 & 1500 CPU request & limit: 1000 & 2000 Xmx of 1000M Java-app keep restarting at random. From Kubernetes configuration perspective, what are the possible reasons for the pod restarts? Ans: OOMKill (Out of Memory) Even though Xmx is set to 1000M, Java memory usage is not limited to heap alone. Native memory (e.g., metaspace, threads, GC, off-heap caches) can exceed this, possibly hitting the pod's memory limit (1500Mi) and triggering OOMKilled. Also there are other possible reason like CPU Throttling and Logrotate Crush Loop or Native Memory Leaks of JVM","title":"3:Case for pod restart issue:"},{"location":"#helm","text":"Please use the accompanied elasticsearch helm template to create a Kubernetes deployment of elasticsearch. Provide a screenshot & deployment yaml of the resultant deployment in Kubernetes.","title":"Helm"},{"location":"#elasticsearch-deployment-on-kubernetes-via-helm","text":"This guide demonstrates how to deploy Elasticsearch using a Helm chart and provides both: The final rendered Deployment YAML A screenshot of the deployed pod in Kubernetes A reference to the GitHub-hosted Helm chart template","title":"Elasticsearch Deployment on Kubernetes via Helm"},{"location":"#github-reference","text":"Helm chart location: View elasticsearch Helm chart on GitHub","title":"GitHub Reference"},{"location":"#metrics","text":"","title":"Metrics"},{"location":"#explain-how-prometheus-work","text":"Ans : Prometheus is tool which uses time series database to scrapes/pulls data from exporters and store it for visualizations and alerting or other type of customizations . Scrape metrics from exporters Prometheus collects metrics from sources like: node_exporter (system metrics) blackbox_exporter (probe endpoints) Store metrics in a time-series database Prometheus stores data in a high-performance, multi-dimensional time-series DB. Labels (key-value pairs) enable powerful filtering and grouping. Pull-based architecture Prometheus pulls metrics from targets. Requires configuration on host machines to expose metrics endpoints (e.g., port 9100 for node_exporter ).","title":"Explain how Prometheus work."},{"location":"#how-do-you-create-custom-prometheus-alerts-and-alerting-rules-for-kubernetes-monitoring-provide-an-example-alert-rule-and-its-configuration","text":"Below there an example alerts rules for kubernetes monitoring . In that rules it is specifically checking nodes availability of kubernetes cluster. - alert: KubernetesNodeNotReady expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0 for: 10m labels: severity: critical annotations: summary: Kubernetes Node not ready (instance {{ $labels.instance }}) description: \"Node {{ $labels.node }} has been unready for a long time\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" Also another example alert for previously shared java application issue - alert: KubernetesContainerOomKiller expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1 for: 0m labels: severity: warning annotations: summary: Kubernetes Container oom killer (instance {{ $labels.instance }}) description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\"","title":"How do you create custom Prometheus alerts and alerting rules for Kubernetes monitoring? Provide an example alert rule and its configuration."},{"location":"#what-is-the-prometheus-query-you-can-use-in-granfana-to-properly-show-usage-trend-of-an-application-metric-that-is-a-counter","text":"Ans: rate(http_requests_total[5m])","title":"What is the Prometheus query you can use in Granfana to properly show usage trend of an application metric that is a counter?"},{"location":"#cassandra","text":"","title":"Cassandra"},{"location":"#casequery-to-db-cluster-returns-different-result-each-time-users-reported-query-result-has-data-records-that-they-deleted-days-ago-explain-what-the-likely-reason-for-the-behavior-and-how-to-avoid-it","text":"Ans: In Cassandra: A DELETE doesn\u2019t remove data immediately.Instead, it writes a tombstone marker.During compaction, tombstones and old data are reconciled and removed.If compaction hasn\u2019t happened, stale data might still exist on disk and be served if consistency is low. Adjust gc_grace_seconds Appropriately Default: 86400 seconds (1 days). Recommendation: Only reduce this value if you're confident that all replicas are consistently up and that repairs are run more frequently than the new gc_grace_seconds value. Setting it too low can risk zombie data if a node misses a tombstone due to being down . How to Set: ALTER TABLE keyspace.table_name WITH gc_grace_seconds = 86400; Ensure Regular RepairsRun nodetool repair on all nodes within the gc_grace_seconds timeframe to synchronize data and tombstones across replicas.This prevents scenarios where a node misses a deletion and later resurrects data . Use Appropriate Consistency Levels. Writes: Use QUORUM or higher to ensure that a majority of replicas acknowledge the write. Reads: Use QUORUM or higher to ensure that reads fetch the most recent data from multiple replicas, reducing the chance of reading stale data.","title":"Case:Query to db cluster returns different result each time. Users reported query result has data records that they deleted days ago. Explain what the likely reason for the behavior and how to avoid it."},{"location":"#mongo","text":"","title":"Mongo"},{"location":"#case","text":"We have a MongoDB replica set called replicaset_1 containing the database sanfrancisco . Collection: sanfrancisco.company_name Due to performance limitations , we've added a second replica set: replicaset_2 Goal: Shard the company_name collection across both replica sets using _id as the shard key","title":"Case"},{"location":"#prerequisites","text":"Ensure: - Sharding is enabled on your MongoDB cluster - Replica sets replicaset_1 and replicaset_2 are added as shards - You are connected to the mongos router and config server is properly running with proper configurations","title":"Prerequisites"},{"location":"#steps-to-shard-the-collection","text":"sh.enableSharding(\"sanfrancisco\") sh.addShard(\"rs-shard-01/shard-01-node-a:27017,shard-01-node-b:27017,shard-01-node-c:27017\") sh.addShard(\"rs-shard-02/shard-02-node-a:27017,shard-02-node-b:27017,shard-02-node-c:27017\") sh.enableSharding(\"sanfrancisco\") sh.shardCollection(\"sanfrancisco.company_name\", { _id: \"hashed\" })","title":"Steps to Shard the Collection"}]}